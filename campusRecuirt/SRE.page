- Nervous?
- 复习吧，投的是分布式存储/分布式计算
- 那Spark论文是不是要好好看一波？
- Hdfs的源码是不是要好好看一下？

- 基础知识
    - 快速排序 写法:

    - ~~~
     int findK(vector<int> &nums, int left, int right, int k) {
        if(left == right) {
            return nums[left];
        }
        
        int l = left, r = right, pivot = nums[l];
        while(l < r) {
            while(l < r && nums[r] >= pivot) {
                --r;
            }
            nums[l] = nums[r];
            while(l < r && nums[l] <= pivot) {
                ++l;
            }
            nums[r] = nums[l];
        }
        nums[l] = pivot;
        if(l - left + 1 >= k) {
            return findK(nums, left, l, k);
        }
        else {
            return findK(nums, l+1, right, k - (l - left + 1));
        }
    }
    ~~~
    这份代码选取最左边位置的数字作为划分的边界， 可以考虑随机化， 然后跟l这个位置的数字进行交换。。
    复杂度问题: 快速排序如果每次划分都等长的话，根据主定理，复杂度为 $O(n^lg(n)$
    这也是下界， 最差的情况是有序， $O(n^2)$


     - 主定理分析一波:
        - $T(n) = aT(n/b) + O(n^d)$, 对每一层的树进行展开计算可得： $\sum_{k=0}^{\log_b n}((a^k * (n/b^k)^d))$, 归纳一下为 $\sum_{k=0}^{\log_b n} ((a/b^d)^k * n^d)$
        - 接下来就变成讨论 $d$ 与 $log_ b a$的关系
        - $d = log_ b a$ $T(n) = O(n^d * \lg n)$
        - $d < log_ b a$ 等比数列公比>1, 所以取最后一项 $T(n) = O(n^(\log_ b a))$
        - $d = log_ b a$ 等比数列公比<1, 所以取第一项 $T(n) = O(n^d)$




- 目前来看， 重点需要准备这几点
    - SQL语法复习
    - java的基本语法， 151条建议
    - Mapreduce 与 Hdfs细节
    - Hbase/Hive尽量多准备
    - 算法

- SRE准备什么呢?
     - 简历上面的相关内容， 主要以项目经验为主
     - Linux内核相关？ 之前课上的几个project
     - Hbase & BigTable论文 (最好把GFS的三篇论文overview一下)

- Content:
     - 项目经验：
          - 华为
          - 2层满拓扑树， 树根和第一层树节点全是由交换机构成，第二层则由宿主机构成。
          - 宿主机上有N个VM组，组内的VM内网络连接， 组头的VM与宿主机Host-only，负责与宿主机通信。
          - 宿主机与同交换机下的相邻节点建立TCP长连接，构成TCP环。
          - 系统启动

          - X-delta增量压缩算法 (源端与目标端逆过程)

          - VM组内传输的策略: 某个虚拟机节点产生了增量之后，首先尽快把增量发给所选的VM组内的组头节点， 是为了能尽快的完成TCP环上以及跨交换机的传输(比较比较耗时), 然后等这部分任务完成之后，开始在VM组内进行传输。(这部分没有必要进行TCP长连接,因为连接比较快，而TCP环是在交换机内的长连接)
          - 考虑宿主机与VM以及VM之间的带宽问题，需要这一部分进行顺序传输(增量是顺序的，进行顺序划分, 搞个流水线)

          - TCP环内的传输是优先的， 为保证传输可靠性，建立TCP长连接。（注意我们这边是将同一个交换机下的物理机器通过网络连接成逻辑上的TCP环
          - 串行双向传输加校验码， 一个明显的优势是， TCP环比较好维护，尤其是在环上有物理节点失效的情况下，可能跳过去跟控制器报告一下即可，然而树形结构会比较蛋疼。

          - 跨交换机传输， 存在带宽共享的问题， 所以尽量减少跨交换机的次数。 控制器进行调度策略(同步进程), 选出所有的TCP环入口节点加入待选队列， 然后把有更新的TCP环的入口节点作为初始遍历节点， 然后进行传输
          - 存在几个节点选取问题， VM组头， TCP环的入口宿节点。 （TCP环入口节点要么是包含更新的宿主机， 要么就是TCP环中资源空闲最多的节点)

         - Route项目
         - 反馈者， 路由器， 每个反馈者负责一些路由器 路由器信息， 路由器连接信息， 指令(包括哪个CIer/Router)
         - Router可以创建反馈者， 可以增加路由器， 一个路由器属于某个反馈者管理， 也可以为某个路由器创建一些命令， 这些命令会写入mysql持久化
         - Router的话可以通过web端来check当前所有的这些信息(包括反馈者包括路由器包括命令有哪些，以及当前命令的执行状态? 已获取？未获取？已返回？
         - Router还提供了一些RESTFUL API接口给反馈者， 他们可以进行登录， 可以修改某个Router的基本信息以及连接信息, 可以获取相关路由器的命令，并且在执行后，通过API返回写入mysql, 这里是反馈者通过post这个API修改了路由器连接信息。
         - 主要是Django学习过程， 从0到1， 完成了全栈的开发， 从前端的设计，到后台的逻辑，接口服务，数据存储， 以及到最后的Nginx+uwsgi的部署都是一个学习的过程。
         - Indroid类似。


     - Linux内核
         - Project2A
              - insmod / rmmod / lsmod

         - Project2B 加一个ctx记录一下该进程被调用的次数， 该进程被调用一次,ctx++
              - /include/linux/sched.h文件task_struct这个结构体加一个ctx变量 kernel/fork.c中对ctx初始化，kernel/sched/core.c中添加代码ctx++
              - proc文件系统是一个伪文件系统，它只存在内存当中，而不占用外存空间。它以文件系统的方式为访问系统内核数据的操作提供接口。用户和应用程序可以通过proc得到系统的信息，并可以改变内核的某些参数。由于系统的信息，如进程，是动态改变的，所以用户或应用程序读取proc文件时，proc文件系统是动态从系统内核读出所需信息并提交的。
          
          - Project3
              - 接受用户输入字符， 来做不同判断， file_operations的一个类型，将用户操作的write与mtest_write进行绑定， 一道用户尝试write proc文件， 那么mtest.ko这个mtest_write就会执行。 注意moudle_init是只会加载一次的。
              - listvma:vma, virtual memory address 显示当前进程的地址空间，start-end, findpage addr，寻找va->pa的映射关系， writeval addr val，向虚拟空间写值。
              - list: mm_struct m, m = current->mm; vas = m->mmap, 然后循环遍历，vas = vas->vm_next; current->m这里边存放的就是该进程使用的内存描述符
              - print_address(current->mm, add);

          - Project4
              - romfs是一个临时的文件系统， 在挂在一个镜像之前先insmod /fs/romfs/romfs.ko 在启动系统后将其加载进内核才能使内核支持romfs文件系统
              - 要完成功能的话，需要去修改fs/romfs/super.c， 实现对挂载的文件系统进行控制， 比如读加密， 隐藏文件等。
              - 然后mount -o loop TEST.IMG mnt, 然后可以到mnt下check

<<<<<<< HEAD

=======
>>>>>>> 04e9fea6a3283896ab9148e19cc895b54073f3c0
## 分布式相关

- Hbase相关
     - 首先为什么需要hbase
     - 半结构化或非结构化数据 对于数据结构字段不够确定或杂乱无章很难按一个概念去进行抽取的数据适合用HBase。以上面的例子为例，当业务发展需要存储author的email，phone，address信息时RDBMS需要停机维护，而HBase支持动态增加.
     - 记录非常稀疏 RDBMS的行有多少列是固定的，为null的列浪费了存储空间。而如上文提到的，HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。
     - 多版本数据  如上文提到的根据Row key和Column key定位到的Value可以有任意数量的版本值，因此对于需要存储变动历史记录的数据，用HBase就非常方便了。比如上例中的author的Address是会变动的，业务上一般只需要最新的值，但有时可能需要查询到历史值。
     - 超大数据量  当数据量越来越大，RDBMS数据库撑不住了，就出现了读写分离策略，通过一个Master专门负责写操作，多个Slave负责读操作，服务器成本倍增。随着压力增加，Master撑不住了，这时就要分库了，把关联不大的数据分开部署，一些join查询不能用了，需要借助中间层。随着数据量的进一步增加，一个表的记录越来越大，查询就变得很慢，于是又得搞分表，比如按ID取模分成多个表以减少单个表的记录数。经历过这些事的人都知道过程是多么的折腾。采用HBase就简单了，只需要加机器即可，HBase会自动水平切分扩展，跟Hadoop的无缝集成保障了其数据可靠性（HDFS）和海量数据分析的高性能（MapReduce）
     
     - Zookeeper(root表) -> root表保存metaData -> metadata保存Region信息
     - 读写过程：client向region server提交写请求
           - region server找到目标region
           - region检查数据是否与schema一致
           - 如果客户端没有指定版本，则获取当前系统时间作为数据版本
           - 将更新写入WAL log
           - 将更新写入Memstore
           - 判断Memstore的是否需要flush为Store文件。
     - 定期执行分裂与合并， 分裂比较简单，现在MemStore里面划分，之后等到compaction的时候在生成Hstore
     - Compaction主要分为Dump以及过多的HStore进行合并

     - [Hbase详解](http://www.ixirong.com/2015/07/16/learn-about-hbase/?utm_source=tuicool&utm_medium=referral)

- 大数据分析的一些常用指标
      - 几种存储模式： Hash/B树/LSM-tree
      - 单机事务 (ACID)
      - 分布式系统 异常/一致性 
      - 衡量指标: 性能/系统吞吐以及响应时间， 可用性, 一致性， 可扩展性
      - CAP/BASE BASE是对CAP中的C/A的平衡结果，即使无法做到强一致性，然而可以采用适当方式达到最终一致性

- 2pc与3pc
    - 2pc
    - 3pc

- MVCC(多版本控制
    - [here](http://www.cnblogs.com/dongqingswt/p/3460440.html)

- Mysql复制的问题
    - [here](http://www.cnblogs.com/zengkefu/p/5716190.html)

- 一致性hash
    - 传统hash分布的缺点在于: 当节点上线下线的时候， N值发生变化， 数据的hash值都发生变化会带来大量的数据迁移
    - 一致性hash就是解决这个问题， 计算每个节点的hash值作为token，有序构成一个hash环， 然后村粗数据的话，计算数据的hash之后， 放到第一个满足hash值大于该数据的hash的节点上。
    - 当需要新增节点之后， 只需要迁移很小一部分数据

    - 缺点是： 只支持随机操作， 不支持范围顺序扫描。 采用类似于B+树的顺序结构。

- HDFS复习关键：
    - 首先是单机容量越来越不足， 需要扩展到分布式系统上。
    - 扩展到分布式系统上之后，就需要考虑各种问题， 如何容错？ 如何设计Master, 如何存储数据等等。
    - Hdfs基于gfs， 所以以gfs为例：
    - GFS主要分为三部分: Master | Chunkserver | client, 其中master主要存储系统元数据，管理文件的命名空间， 还负责全局的系统控制， 比如lease机制， 比如心跳检测和负载均衡。    
Chunkserver会管理chunk数据， client是接受用户读写请求操作， 进行处理。
    - 关键问题： 
          - 单Master的设计， 原因是单Master设计操作比较简单， 比如不需要进行数据同步与数据一致性的考虑。 但是需要注意不要让单master成为单点故障的源点。 所以在GFS中采用了一些机制， 比如Lease（租约机制）
就是将控制权交给chunkserver，来代理Master来完成读写控制。
          - 如何容错？ 对于Master而言， 采用日志 & checkpoint方式。修改Master里面保存的元数据信息等都需要先写log.   对于chunkServer而言， 那就是多副本技术。 如果出现副本丢失， Master会自动启动恢复。
          - 一致性？  采用弱一致性， append only.
          - ChunkServer的设计? 一个chunk多大比较好? chunk太大，可以利用局部性减小交互，减少元数据数量。 缺点会成为hotSpot.
          - Client设计？ 仅缓存从Master读过来的元数据信息， 不缓存chunk Data. 而且Master也不会保存chunk信息， 会在每次系统启动之后， 通过心跳进行数据交换。
    - 读流程：
          - Client向Master查询Chunk信息， Master将chunk的具体信息返回给Client
          - Client根据获取到的位置信息， 访问Chunkserver获取Chunk Data.
    - 写流程:
          - Master管理Lease， 如果没有ChunkServer在lease的话， 将Lease的权限给其中一台。
          - Client向Master查询Chunk信息， Master将chunk的具体信息返回给Client
          - Client根据获取到的位置信息， 将要写入(追加的)的数据放到一个chunkServer上， 然后进行数据流水线传输(里面有些选择机制， 比如优先放到比较近的机器上)
          - 当所有副本都确认收到了数据，就发一个写请求控制命令给Lease机制中选出的Primary node，Primary node将这些写请求发给其他node, 然后进行按顺序读写。 

- GFS的一些关键点
    - 机器出故障很正常
    - Client -> Master -> chunkServer
    - Master上有chunk的元数据信息， 然而并不会缓存， 而是通过 心跳来定时交换
    - Client不会缓存文件数据， 只会缓存从Master获得的元数据
    - Chunk的大小选择问题
    - 解决单点故障， 用 lease机制选择primary node的问题
    - 数据流与控制流分离， 以及写过程
    - 容错问题， Master采用日志 + checkPoint保存修改信息。 ChunkServer多副本

    - 负载均衡问题
    - COW的snapshot

- [关于Hive如何将SQL转为mapReduce操作](http://www.cnblogs.com/yaojingang/p/5446310.html)

- CAP原理 / Base原理
    - [here](https://my.oschina.net/foodon/blog/372703)

- 单机存储引擎
    - Hash存储引擎
    - B/B+树， mysql innodb引擎
    - LSM-tree Bigtable采用了类似的技术， 大概情况 : 将对数据修改的增量保持在内存中，达到一定条件批量写入磁盘(随机写转换为顺序写)， 然而读取的时候需要合并磁盘上与内存中的内容。
    - 内存中主要包括: Memtable 和 不可变Memtable, 磁盘上主要包括: 当前文件， 清单(manifest)文件， 操作日志, SSTable文件， 当然每次写入内存之前都要先写入log

- RDMBS的一些挑战：
    - 事务， 分布式事务很难搞定。。2pc/3pc等一致性协议性能比较低。
    - 数据冗余， 传统的范式设计有时候并不方便。。(比如需要多联表)
    - B树的存储引起 更新性能不如LSM等， 也不如专门定制的Key-value存储

- Snapshot的理解
    - 应该是指在原数据未丢失的基础上， 快速恢复到某个时间点的状态
    - COW模式， 写时复制， 也就是说创建snapshot的时候，只是放一些引用指向， 只有当文件真正被修改的时候，才会copy一份出来作为一个瞬时状态， 之后的修改会在copy出来的地方修改。


- Supplement
    - 编译 -> 连接 -> 运行
    - [编译的几个过程](http://www.360doc.com/content/13/1216/09/8728596_337509108.shtml)

    - 常见的几种hash算法

    - Hard Link - > soft Link [here](http://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/)

    - [排序的几种策略](http://blog.chinaunix.net/uid-21457204-id-3060260.html)

    - [部分参考例题](https://zhuanlan.zhihu.com/p/23284834)
    
    - 需要了解常见的指标， 比如SSD读写速度， 网络传输时延， 磁盘的年故障率
   
    - 常见的存储引擎（Hash存储， B树存储， LSM存储， 
